{"pred": "The ground truth for fake news is established by a human annotator.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Ground truth is not established in the paper"], "all_classes": null, "length": 3141}
{"pred": "GhostVLAD is an extension of the NetVLAD approach, which was proposed for face recognition by Y. Zhong [10]. GhostVLAD works exactly similar to NetVLAD except it adds (typically 2-4) ghost clusters along with the NetVLAD clusters.\n\nAnswer: The Ghost clusters are added to map any noisy or irrelevant content into ghost clusters and are not included during the feature aggregation stage.\n\nAnswer: GhostVLAD is an extension of the NetVLAD approach, which was proposed for face recognition by Y. Zhong [", "answers": ["extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters", "An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content."], "all_classes": null, "length": 2454}
{"pred": "The proposed model outperforms the state-of-the-art results by 68.8% to 71.8%.\n\nQuestion: What is the name of the dataset used in the experiment?\n\n\nAnswer: The IEMOCAPARE model is used to predict the emotion class.\n\nAnswer: The IEMOCAP model is used to predict the emotion class.\n\n\nAnswer: The proposed model outperforms the state-of-the-the-the-the-the-the-the-the-the-the-the-the-the-", "answers": ["the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)"], "all_classes": null, "length": 3207}
{"pred": "The article proposes the use of context tweets as additional features for neural network models.\n\nQuestion: What is the dataset used?\n\nAnswer: The dataset used is Hate and Abusive Speech on Twitter BIBREF10 .\n\nQuestion: What is the baseline accuracy of the neural network models?\n\nAnswer: The baseline accuracy of neural network models is as follows:\n\nCNN: 0.316\n\nRNN: 0.316\n\nPark and Fung park2017one proposed a HybridCNN model which out", "answers": ["using tweets that one has replied or quoted to as contextual information", "text sequences of context tweets"], "all_classes": null, "length": 2060}
{"pred": "They looked at FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post, Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure, Cooking Adventure", "answers": ["FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney", "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."], "all_classes": null, "length": 3411}
{"pred": "No, the hashtag dataset contains both English and non-English data.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data?\n\nAnswer: No, the hashtag dataset contains both English and non-English data.\n\nQuestion: Do the hashtag and SemEval datasets contain only English data?\n\nAnswer: No, the hashtag dataset contains both English and non-English data.\n\n\nQuestion: Do the hashtag and SemEval datasets contain only English data?\n\nAnswer: No, the hashtag and SemEval", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3735}
{"pred": "The task is defined as a summarization task, where the output is a concept map.\n\nQuestion: What is the size limit for the concept maps?\n\nAnswer: The size limit is 25 concepts and 25 relations.\n\n\nQuestion: What is the size limit for the concept maps?\n\nAnswer: The size limit is 25 concepts and 25 relations.\n\nQuestion: What is the topic?\n\nAnswer: The topic is the is the topic.\n\nQuestion: What is the topic?\nAnswer: The topic is the is the\n\nQuestion: What is", "answers": ["Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2"], "all_classes": null, "length": 4263}
{"pred": "We evaluate our models on three datasets: CNN/DailyMail, NYT, and XSum.\n\nQuestion: What is the model architecture?\n\nAnswer: We use a pretrained language modeling objective of masked language modeling the question.\n\n\nQuestion: What is the model architecture?\n\n\nAnswer: We use a pretrained modeling objective of masked language modeling and next-sentence prediction and next-sentence prediction and selective summarization.\n\n\nAnswer: We use a encoder-decoder architecture, combining both extractive and abstractive modeling form", "answers": ["CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum", "the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22"], "all_classes": null, "length": 4369}
{"pred": "The proposed approach is different from other WSD approaches employing word embeddings in the following ways:\n\n1. The proposed approach uses a variant of max-margin objective based on the asymmetric KL divergence energy function to capture lexical inference relations such as causality (for example, funny $\\rightarrow$ hilarious) and hypernymy (for example, dog $\\rightarrow$ animal).\n\n2. The proposed approach uses a variant of max-margin objective based on the asymmetric KL divergence energy function to capture polysemous words.\n\n3. The proposed approach uses a", "answers": ["GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."], "all_classes": null, "length": 2189}
{"pred": "The model is trained on a large corpus of text-comprehension questions and answers. The model is then applied to a new question-answering task.\n\nQuestion: What is the model architecture?\n\n\nAnswer: The model is a recurrent neural network.\n\n\nQuestion: What is the model?\nAnswer: The model is a recurrent neural network.\n\nQuestion: What is the model architecture?\nAnswer: The model is a recurrent network.\n\n\nQuestion: What is the model?\nAnswer: The model is a recurrent network.\n\nQuestion: What is", "answers": ["simply averaging the predictions from the constituent single models"], "all_classes": null, "length": 4212}
{"pred": "The source of the dataset is Friends and EmotionPush.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 1,000 dialogues for both Friends and EmotionLines dataset.\n\n\nQuestion: What is the size of the dataset?\n\nAnswer: The size of the dataset is 1,0000000000000000000000000000000000000000000000000", "answers": ["Friends TV sitcom, Facebook messenger chats"], "all_classes": null, "length": 3185}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English", "Simple English"], "all_classes": null, "length": 2243}
{"pred": "IMDb dataset\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer: w4s0h0 for SW and w4s1h0 for BW\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer: w4s0h0 for SW and w4s1h0 for BW\n\nQuestion: What is the optimal combination of word2vec hyper-parameters for intrinsic and extrinsic NLP purposes?\n\nAnswer:", "answers": ["IMDb dataset of movie reviews", "IMDb"], "all_classes": null, "length": 2327}
{"pred": "The proposed system achieves +6.12 F1 improvement on DL-PS, +4.51 F1 improvement on EC-MT, and +9.19 F1 improvement on EC-UQ.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["F1 scores of 85.99 on the DL-PS data,  75.15 on the EC-MT data and 71.53 on the EC-UQ data ", "F1 of 85.99 on the DL-PS dataset (dialog domain); 75.15 on EC-MT and 71.53 on EC-UQ (e-commerce domain)"], "all_classes": null, "length": 5310}
{"pred": "Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\n\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer", "answers": ["No"], "all_classes": null, "length": 3445}
{"pred": "The dataset used is the one from the article.\n\nQuestion: What is the purpose of the system?\n\nAnswer: The purpose of the system is to provide a conversational system that can be used to provide a conversational system that can be used to provide a conversational system that can be used to provide a conversational system that can be used to provide a conversational system that can be used to provide a conversational system that can be used to provide a conversational system that can be used to provide a conversational system that can be used to provide a conversational system that can be used to provide a conversational system", "answers": ["Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.", "a self-collected financial intents dataset in Portuguese"], "all_classes": null, "length": 13401}
{"pred": "The best performance was achieved by the Health Care sector.\n\nAnswer: The best performance was achieved by the Health Care sector.\n\nAnswer: The best performance was achieved by the Health Care sector.\n\n\nAnswer: Which stock market sector achieved the best performance was achieved by the Health Care.\n\nAnswer: The best performance was achieved by the Health Care sector.\n\nAnswer: The best performance was achieved by the Health Care sector.\n\n\nAnswer: The best performance was achieved by the Health Care.\n\nAnswer: The best performance was achieved by the Health Care.\n\nAnswer: The best performance was", "answers": ["Energy with accuracy of 0.538", "Energy"], "all_classes": null, "length": 10349}
{"pred": "RNN-based NMT, Transformer-based NMT, and SMT.\n\n Question: what is the size of the dataset?\n\n Answer: 1.24M bilingual sentence pairs.\n\n\n Question: what is the size of the dataset?\n\n Answer: 1.248M bilingual sentence pairs.\n\n Question: what is the size of the dataset?\n\n Answer: 1.24M bilingual sentence pairs.\n\n Question: what is the size of the dataset?\n Answer: 1.2M bilingual sentence", "answers": ["RNN-based NMT model, Transformer-NMT"], "all_classes": null, "length": 3708}
{"pred": "The three regularization terms are neutral features, maximum entropy, and KL divergence.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"yes\", \"no\", or \"unanswerable\".\n\n\nAnswer: The proposed methods are robustness problem in the framework of GE framework of GE-FL.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution", "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution"], "all_classes": null, "length": 3604}
{"pred": "The baselines are the SVM, CNN, and RCNN models.\n\nQuestion: What is the FBFans dataset?\n\nAnswer: The FBFans dataset is a dataset of 100,00 as the number of posts as the number of words.\n\nQuestion: What is the FBFans dataset?\n\nAnswer: The FBFans dataset is a dataset of 100 as the number of posts as the number of words as the number of words as the number of words as the number of words as the number of words as the number of words as the number of words as", "answers": ["SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information", "SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information"], "all_classes": null, "length": 4512}
{"pred": "By 10%\n\nQuestion: What is the name of the company?\n\nAnswer: Twitter\n\nQuestion: What is the name of the paper?\n\n\nAnswer: Twitter Sentiment Classification with 5-worded.\n\n\nQuestion: What is the name of the paper?\n\nAnswer: Twitter Sentiment Classification with 5-worded.\n\nQuestion: What is the name of the paper?\n\n\nAnswer: Twitter\n\n\nQuestion: What is the name of the paper?\n\nAnswer: Twitter\n\nQuestion: What is the name of the paper?\nAnswer", "answers": ["They decrease MAE in 0.34"], "all_classes": null, "length": 2735}
{"pred": "The Transformer architecture BIBREF0 has been shown to be a powerful model for NLP tasks. However, the Transformer's attention mechanism is a black box, and it is unclear how the attention weights are learned. In this work, we show that the attention weights are sparse.\n\nAnswer: The Transformer architecture BIBREF0 has been shown to be a powerful model for NLP tasks. However, the Transformer architecture is a powerful model for NLP tasks. However, the Transformer is a powerful model for NLP tasks. However, the Transformer is a powerful model for NLP tasks. However", "answers": ["the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence", "We introduce sparse attention into the Transformer architecture"], "all_classes": null, "length": 4902}
{"pred": "the baseline is the sentence-level translation of the context-agnostic MT system.\n\nQuestion: what is the DocRepair model?\n\nAnswer: the DocRepair model is a monolingual sequence-to-context-level repair system that uses monolingual sentence-level monoling sentence-level monolingual sentence-level monolingual sentence-level monolingual sentence-level monolingual sentence-level monolingual sentence-level monolingual sentence-level monolingual sentence-level monolingual sentence-level monoling", "answers": [" MT system on the data released by BIBREF11", "Transformer base, two-pass CADec model"], "all_classes": null, "length": 3716}
{"pred": "The article does not provide any metrics for evaluation.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. If the question cannot be answered based on the information in the article, write \"unanswerable\".\n\n\nAnswer:\n\n\nAnswer:\n\nAnswer:\n\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\n", "answers": ["translation probabilities, Labeled Attachment Scores (LAS)", "accuracy, Labeled Attachment Scores (LAS)"], "all_classes": null, "length": 3402}
{"pred": "The attention module is pretrained on the MT dataset.\n\nQuestion: What is the role of the attention module?\n\nAnswer: The attention module is used to align the source and target text.\n\n\nQuestion: What is the role of the attention module?\n\nAnswer: The attention module is used to align the source and target text.\n\nQuestion: What is the role of the attention module?\n\nAnswer: The attention module is used to align the source and target.\n\n\nQuestion: What is the attention module?\nAnswer: The attention module is used to align the source and target", "answers": ["the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage."], "all_classes": null, "length": 4656}
{"pred": "The cognitive features, along with textual features used in best available sarcasm detectors, are used to train a binary classifier against given sarcasm labels.\n\nQuestion: What is the effectiveness of the cognitive features?\n\n\nAnswer: The accuracy of the sarcasm detector is improved by 3.7.\n\n\nQuestion: What is the effectiveness of the cognitive features?\n\n\nAnswer: The cognitive features are not.\n\n\nQuestion: What is the effectiveness of the effectiveness of the cognitive features?\n\nAnswer: The cognitive features", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The encoder is a character-based encoder-decoder.\n\nQuestion: What is the auxiliary objective of the system?\n\nAnswer: The auxiliary objective is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of the multi-lingual multi-tasking approach?\n\nAnswer: The multi-lingual multi-tasking approach outperforms the baseline by 18.30%.\n\nQuestion: What is the effect of the multi-lingual multi-tasking approach for the English test set?\n\nAnswer: The multi-", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\n\n\nAnswer: Yes\n\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes", "answers": ["Unanswerable", "Yes"], "all_classes": null, "length": 6391}
{"pred": "\n\nThe baselines are:\n\n1. A conventional acoustic model with a 10-m Mel-filterbank and a 10-m pitch-filterbank.\n\n2. A conventional acoustic model with a 10-m Mel-filterbank and a 10-m pitch-filterbank, and a 10-m pitch-filterbank.\n\n3. A conventional acoustic model with a 10-m Mel-filterbank and a 10-m pitch-filterbank, and a 10-m pitch-filter", "answers": ["Unanswerable", "LF-MMI Attention\nSeq2Seq \nRNN-T \nChar E2E LF-MMI \nPhone E2E LF-MMI \nCTC + Gram-CTC"], "all_classes": null, "length": 1856}
{"pred": "Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\n\nAnswer: Unanswerable\n\n\nQuestion: What is the name of the company?\n\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\n\nAnswer: Unanswerable\n\nQuestion: What is the name of the company?\n\nAnswer: Unanswerable\n\nQuestion: What is the", "answers": ["22,880 users", "20,000"], "all_classes": null, "length": 4160}
{"pred": "We use BLEU-1, BLEU-4, ROUGE-L, and Distinct-1/2.\n\nQuestion: What is the dataset size?\n\nAnswer: 180K recipes and 180K recipes.\n\nQuestion: What is the dataset size?\nAnswer: 180K recipes.\n\nQuestion: What is the dataset split?\nAnswer: 10000.\n\nQuestion: What is the dataset split?\nAnswer: 0000.\n\nQuestion: What is the dataset split?", "answers": ["Byte-Pair Encoding perplexity  (BPE PPL),\nBLEU-1,\nBLEU-4,\nROUGE-L,\npercentage of distinct unigram (D-1),\npercentage of distinct bigrams(D-2),\nuser matching accuracy(UMA),\nMean Reciprocal Rank(MRR)\nPairwise preference over baseline(PP)", "BLEU-1/4 and ROUGE-L, likelihood of generated recipes using identical input specifications but conditioned on ten different user profiles, user matching accuracy (UMA), Mean Reciprocal Rank (MRR), neural scoring model from BIBREF33 to measure recipe-level coherence", " Distinct-1/2, UMA = User Matching Accuracy, MRR\n= Mean Reciprocal Rank, PP = Pairwise preference over baseline (evaluated for 310 recipe pairs per model)"], "all_classes": null, "length": 2673}
{"pred": "The labels are:\n\n(1) The symptom is mentioned in the dialogue.\n\n(2) The symptom is not mentioned in the dialogue.\n\n\n(3) The symptom is mentioned in the dialogue.\n\n\n\n\n\n(4) The symptom is not mentioned in the dialogue.\n\n\n(5) The symptom is mentioned in the dialogue.\n\n\n(6) The symptom is not mentioned in the dialogue.\n\n\n(7) The symptom is mentioned in the dialogue.\n\n\n(8) The sympt", "answers": ["(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer", "the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms"], "all_classes": null, "length": 3424}
{"pred": "The task-specific encoder needs to be trained on data that is annotated by experts.\n\nQuestion: How does the task-specific encoder differ from the universal encoder?\n\n\nAnswer: The task-specific encoder is trained on data that is annotated by experts.\n\nQuestion: What is the task-specific encoder used for the task-specific encoder?\n\nAnswer: The task-specific encoder is used for the task-specific encoder.\n\nQuestion: What is the task-specific encoder used for the task-specific encoder?\nAnswer: The", "answers": ["57,505 sentences", "57,505 sentences"], "all_classes": null, "length": 4371}
{"pred": "The Transformer architecture BIBREF0 is a sequence-to-sequence (seq2seq) model which maps an input sequence to an output sequence through hierarchical multi-head attention mechanisms. It contrasts with previous seq2seq2seq2seq2seq models such as gated attention heads BIBREF0, the “sparse, while in this work.\n\nAnswer: The Transformer architecture BIBREF0 is a sequence-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to", "answers": ["four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German", " four machine translation tasks, IWSLT 2017 German $\\rightarrow $ English BIBREF27, KFTT Japanese $\\rightarrow $ English BIBREF28, WMT 2016 Romanian $\\rightarrow $ English BIBREF29, WMT 2014 English $\\rightarrow $ German BIBREF30"], "all_classes": null, "length": 4898}
{"pred": "The improvement in performance for Estonian in the NER task is significant.\n\nQuestion: What is the improvement in performance for Latvian in the NER task?\n\nAnswer: The improvement in performance for Latvian is significant.\n\n\nQuestion: What is the improvement in performance for Latvian is significant.\n\nAnswer: The improvement in performance for Latvian is significant.\n\nQuestion: What is the improvement in performance for Latvian is significant.\n\nAnswer: The improvement in performance for Latvian is significant.\n\nQuestion: What is the improvement in performance for Latv", "answers": ["5 percent points.", "0.05 F1"], "all_classes": null, "length": 3290}
{"pred": "They are both computer scientists.\n\nQuestion: What is the purpose of the article?\n\nAnswer: To describe the research process of computational text analysis.\n\n\nQuestion: What is the article about?\n\n\nAnswer: It is a description of the research process of the research questions, the research process of the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research questions, the research", "answers": ["Unanswerable"], "all_classes": null, "length": 8506}
{"pred": "Yes, LDA is an unsupervised method.\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-built dataset.\n\n\nQuestion: What is the difference between the two datasets?\n\nAnswer: The Honeypot dataset is a public dataset, and the Weibo dataset is a self-built dataset, and the Hone is a self-built dataset, and the Weib dataset, and the Weibo dataset is a public dataset, and the Weibo dataset", "answers": ["No", "No"], "all_classes": null, "length": 2239}
{"pred": "The languages of South Africa are similar to each other.\n\nQuestion: Which languages are dissimilar to each other?\n\nAnswer: The languages of South Africa are dissimilar to each other.\n\nQuestion: Which languages are similar to English?\n\nAnswer: The languages of South Africa are similar to English.\n\nQuestion: Which languages are dissimilar to English?\n\nAnswer: The languages of South Africa are dissimilar to English.\n\nQuestion: Which languages are similar to each other?\n\nAnswer: The languages of South Africa are similar to each other.\n\nQuestion: Which languages are dissimilar", "answers": ["Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)", "The Nguni languages are similar to each other, The same is true of the Sotho languages"], "all_classes": null, "length": 1877}
{"pred": "They compared with 6-layers and 9-layers models.\n\nQuestion: what is the difference between the two models?\n\nAnswer: The 9-layers model is deeper than the 6-layers model.\n\n\nQuestion: what is the 9-layers model?\n\nAnswer: The 9-layers model is deeper than the 6-layers model.\n\nQuestion: what is the 9-layers model?\n\nAnswer: The 9-layers model.\n\n\nQuestion: what is the 9-layers model?\nAnswer: The 9-layers model", "answers": ["Unidirectional LSTM networks with 2, 6, 7, 8, and  9 layers."], "all_classes": null, "length": 4286}
{"pred": "The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\n\nAnswer: The data set is large.\n\n\nAnswer", "answers": ["a sample of  29,794 wikipedia articles and 2,794 arXiv papers "], "all_classes": null, "length": 4187}
{"pred": "The human judgements were assembled by the authors of the article.\n\nQuestion: What is the purpose of the article?\n\nAnswer: The purpose of the article is to provide a detailed description of the RNNMorphological segmentation and attention model for the translation of the Tamil language.\n\nQuestion: What is the article about?\nAnswer: The article is about the translation of the Tamil language.\n\nQuestion: What is the article about the translation?\nAnswer: The article is about the translation of the Tamil.\nQuestion: What is the article about the article?\nAnswer:", "answers": ["50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.", "adequacy, precision and ranking values"], "all_classes": null, "length": 5344}
{"pred": "Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\n\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\n\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\nAnswer: Yes\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4472}
{"pred": "The models are evaluated by measuring the accuracy of the decoder in reconstructing the target sentence from the keywords.\n\nAnswer: The models are evaluated by measuring the accuracy of the decoder in reconstructing the target sentence from the keywords.\n\nAnswer: The models are evaluated by measuring the accuracy of the decoder in reconstructing the target sentence from the keywords.\n\nAnswer: The models are evaluated by measuring the accuracy of the decoder in reconstructing the target sentence from the keywords.\n\nAnswer: The models are evaluated by measuring the accuracy of the decoder in reconstruct", "answers": ["by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews", "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence"], "all_classes": null, "length": 1873}
{"pred": "The evaluation metrics are accuracy, precision, recall, F1 score, and confusion matrix.\n\nAnswer: The evaluation metrics are accuracy, precision, recall, F1 score, and confusion matrix.\n\n\nAnswer: What evaluation metrics are looked at for classification tasks?\n\n\nAnswer: The evaluation metrics are accuracy, precision, recall, F1 score, and confusion matrix.\n\n\nAnswer: The evaluation metrics are accuracy, precision, recall, F1 score, and confusion matrix.\n\n\nAnswer: The evaluation metrics are accuracy, precision, recall, F1 score, and confusion matrix.\n\n\nAnswer", "answers": ["Precision, Recall, F-measure, accuracy", "Precision, Recall and F-measure"], "all_classes": null, "length": 3044}
{"pred": "The source domain is the domain of the source domain, and the target domain is the domain of the target domain.\n\nQuestion: What is the problem?\n\nAnswer: The problem is to reduce the domain discrepancy.\n\n\nQuestion: What is the problem?\n\nAnswer: The problem is the problem.\n\nQuestion: What is the problem?\nAnswer: The problem is the problem.\n\nQuestion: What is the problem?\nAnswer: The problem.\nQuestion: What is the problem?\nAnswer: The problem is the problem.\nQuestion: What is the problem?\nAnswer", "answers": ["Book, electronics, beauty, music, IMDB, Yelp, cell phone, baby, DVDs, kitchen", "we use set 1 of the source domain as the only source with sentiment label information during training, and we evaluate the trained model on set 1 of the target domain, Book (BK), Electronics (E), Beauty (BT), and Music (M)"], "all_classes": null, "length": 5061}
{"pred": "LSTMs and GRUs.\n\nQuestion: what is the difference between the LSTM and the PRU?\n\nAnswer: The PRU uses the pyramidal transformation to learn representations at higher dimensionality with fewer parameters.\n\n\nQuestion: what is the PRU?\n\nAnswer: the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU is the PRU", "answers": ["Variational LSTM, CharCNN, Pointer Sentinel-LSTM, RHN, NAS Cell, SRU, QRNN, RAN, 4-layer skip-connection LSTM, AWD-LSTM, Quantized LSTM"], "all_classes": null, "length": 3319}
{"pred": "NeuronBlocks includes a gallery of neural network modules, including word/character embedding, CNN, BiLSTM, Attention, etc.\n\nQuestion: What is the purpose of NeuronBlocks?\n\nAnswer: NeuronBlocks is a DNN toolkit for NLP tasks, which provides a two-layer solution to satisfy the requirements from three types of engineers.\n\nQuestion: What are the advantages of NeuronBlocks?\n\nAnswer: NeuronBlocks provides a two-layer solution to satisfy the requirements from three types of engineers.\n\nQuestion", "answers": ["Embedding Layer, Neural Network Layers, Loss Function, Metrics", "Embedding Layer, Neural Network Layers, Loss Function, Metrics"], "all_classes": null, "length": 1678}
{"pred": "The standard orthography of the 311 languages in the Wiktionary training corpus was used.\n\nQuestion: what is the purpose of the artificial token?\n\n\nAnswer: The artificial token is used to facilitate the sharing of the language.\n\n\nQuestion: what is the purpose of the artificial token?\n\n\nAnswer: the artificial token is the purpose of the artificial token is the purpose of the artificial token is the purpose of the artificial token is the purpose of the artificial token is the purpose of the artificial token is the purpose of the purpose of the purpose of the purpose of the purpose of the", "answers": ["the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary", "multilingual pronunciation corpus collected by deri2016grapheme"], "all_classes": null, "length": 3244}
{"pred": "The baselines are the speculation detection and scope resolution subtasks of the BioScope Corpus.\n\nQuestion: What were the models?\n\nAnswer: The models are BERT, XLNet and RoBERTa.\n\nQuestion: What were the results?\n\nAnswer: The results are reported in the paper.\n\nQuestion: What were the conclusions?\n\nAnswer: The conclusions are that XLNet outperforms RoBERTa on the speculation detection and scope resolution task.\n\nQuestion: What were the limitations?\n\nAnswer: The limitations are that the models are", "answers": ["varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)"], "all_classes": null, "length": 2215}
{"pred": "English\n\nQuestion: What is the dataset they use?\n\nAnswer: XNLI\n\nQuestion: What is the training set?\n\nAnswer: English\n\n\nQuestion: What is the test set?\n\n\nAnswer: English\n\n\nQuestion: What is the translation?\n\nAnswer: English\n\n\nQuestion: What is the test set?\n\nAnswer: English\n\nQuestion: What is the translation artifacts?\n\nAnswer: Yes\n\n\nQuestion: What is the translation artifacts?\nAnswer: No\n\nQuestion: What is the translation artifacts?\nAnswer:", "answers": ["English\nFrench\nSpanish\nGerman\nGreek\nBulgarian\nRussian\nTurkish\nArabic\nVietnamese\nThai\nChinese\nHindi\nSwahili\nUrdu\nFinnish", "English, Spanish, Finnish"], "all_classes": null, "length": 4086}
{"pred": "They test their method on the task of predicting hashtags for a given tweet.\n\nQuestion: What is the size of the vocabulary?\n\nAnswer: The vocabulary is the set of all unique words in the training data.\n\nQuestion: What is the size of the character look-up table?\n\nAnswer: The character look-up table is the set of all unique characters in the training data.\n\nQuestion: What is the size of the word look-up table?\n\n\nAnswer: The word look-up table is the set of all unique words in", "answers": ["None"], "all_classes": null, "length": 2473}
{"pred": "Yes\n\nQuestion: Do they use a bifocal attention mechanism?\n\nAnswer: Yes\n\nQuestion: Do they use a gated orthogonalization mechanism?\n\nAnswer: Yes\n\n\nQuestion: Do they use a fused bifocalization mechanism?\n\nAnswer: Yes\n\nQuestion: Do they use a fused bifocalization mechanism?\nAnswer: Yes\n\nQuestion: Do they use a gated orthogonalization mechanism?\nAnswer: Yes\n\n\nQuestion: Do they use a gated orthogonalization mechanism?\nAnswer: Yes\n\nQuestion: Do they use a bif", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4457}
{"pred": "Yes, PolyResponse was evaluated against a baseline that uses a slot-based ontology and a slot-value ontology.\n\nAnswer: No, PolyResponse was not evaluated against a baseline that uses a slot-value ontology.\n\n\nAnswer: No, PolyResponse was not evaluated against a baseline that uses a slot-value ontology and a slot-based ontology.\n\nAnswer: No, PolyResponse was not evaluated against a baseline that uses a slot-value ontology.\n\n\nAnswer: No, PolyResponse was not evaluated against a baseline that uses a slot-based", "answers": ["No", "No"], "all_classes": null, "length": 2738}
{"pred": "They use the Linguistic Inquiry and Word Count (LIWC) BIBREF12 .\n\nAnswer: They use the Meaning Extraction Method (MEM) BIBREF13 .\n\nAnswer: They use the Meaning Extraction Method (MEM) BIBREF14 .\n\nAnswer: They use the Meaning Extraction Method (MEM) BIBREF15 .\n\nAnswer: They use the Meaning Extraction Method (MEM) BIBREF16 .\n\nAnswer: They use the Meaning Extraction Method (MEM) B", "answers": ["using the Meaning Extraction Method", "Unanswerable"], "all_classes": null, "length": 1440}
{"pred": "The best-performing system aims to identify argument components in the pathos dimension of argumentation. The components are:\n\n1. Claim\n2. Premise\n3. Backing\n4. Rebuttal\n5. Refutation\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["claim, premise, backing, rebuttal, and refutation", "claim, premise, backing, rebuttal, refutation"], "all_classes": null, "length": 14472}
{"pred": "1-grams\n\nQuestion: Which of the following is the best metric for evaluating table-to-text generation?\n\nAnswer: PARENT\n\nQuestion: Which of the following is the best metric for evaluating table-to-text generation?\n\nAnswer: PARENT\n\nQuestion: Which of the following is the best metric for evaluating table-to-text generation?\n\nAnswer: PARENT\n\nQuestion: Which of the following is the best metric for evaluating table-to-text?\n\nAnswer: PARENT\n\nQuestion: Which of the following", "answers": ["Unanswerable", "Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4"], "all_classes": null, "length": 3827}
{"pred": "The Twitter dataset contains 100,000 tweets.\n\n Question: How large is the OSG dataset?\n\n Answer: The OSG dataset contains 1,000,000,00,00,00,00,00,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,", "answers": ["1,873 Twitter conversation threads, roughly 14k tweets", "1,873 Twitter conversation threads, roughly 14k tweets"], "all_classes": null, "length": 3721}
{"pred": "The 12 languages covered are English, French, German, Spanish, Italian, Portuguese, Russian, Mandarin, Japanese, Korean, and Yue Chinese.\n\nQuestion: What is the dataset?\n\n\nAnswer: The lack of a single word.\n\n\nQuestion: What is the 1,2, 3, 4, 5, 6, 7, 8, 9, 10, 111, 12, 3, 4, 5, 6, 7, 8, 9, 10, 1", "answers": ["Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese", "Chinese Mandarin, Welsh, English, Estonian, Finnish, French, Hebrew, Polish, Russian, Spanish, Kiswahili, Yue Chinese"], "all_classes": null, "length": 14660}
{"pred": "Wikipedia Talk Page and Reddit ChangeMyView\n\n Question: What is the name of the model?\n\nAnswer: CRAFT\n\n Question: What is the model's objective?\n\n\nAnswer: To forecast the future-conversational dynamics.\n\n\n Question: What is the model's forecasting task?\n\nAnswer: Conversational events\n\n\n Question: What is the model's architecture?\n\nAnswer: online fashion\n\n\n Question: What is the model's objective?\nAnswer: To process the conversation as it develops and fine-t", "answers": [" `Conversations Gone Awry' dataset, subreddit ChangeMyView", "An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. "], "all_classes": null, "length": 4718}
{"pred": "No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.", "answers": ["No", "No"], "all_classes": null, "length": 2276}
{"pred": "The quality of the data is evaluated by comparing the BLEU scores of the models trained on the data with the BLEU scores of the models trained on the WMT14 English-to-English ST corpus.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets", "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations"], "all_classes": null, "length": 2435}
{"pred": "They use a dual recurrent encoder model that encodes the information from audio and text sequences using two RNNs and then combines the information from these sources using a feed-forward neural network to predict the emotion class.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["combines the information from these sources using a feed-forward neural model", "encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model"], "all_classes": null, "length": 3201}
{"pred": "by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nAnswer: by 6.37 BLEU\n\nAnswer: by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nAnswer: by 6.37 BLEU\n\nAnswer: by 2.11 BLEU, 1.7 FKGL and 1.07 SARI\n\nAnswer: by 6.37 BLEU\n\nAnswer: by ", "answers": ["For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.\nFor the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU.", "6.37 BLEU"], "all_classes": null, "length": 2271}
{"pred": "100\n\nAnswer: 100\n\nAnswer: 100\n\nAnswer: 100\n\nAnswer: 100\n\n\nAnswer: how many humans evaluated the results?\n\n\nAnswer: 1000\n\nAnswer: 100\n\nAnswer: 100\n\nAnswer: 100\n\nAnswer: 100\n\n\nAnswer: 10\n\nAnswer: 10\n\nAnswer: 10\n\nAnswer: 10\n\nAnswer: 10\n\n", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3711}
{"pred": "Tweets going viral are those that have been retweeted more than 1000 times.\n\nQuestion: What is the distribution of the number of retweets for viral tweets containing fake news and not containing them?\n\n\nAnswer:\n\n\nQuestion: What is the distribution of the number of retweets containing fake news and not containing fake news?\n\nAnswer: The distribution of the number of retweets containing fake news is skewed.\n\n\nQuestion: What is the distribution of the number of retweets containing fake news?\n\nAnswer: The distribution of ret", "answers": ["Viral tweets are the ones that are retweeted more than 1000 times", "those that contain a high number of retweets"], "all_classes": null, "length": 3144}
{"pred": "CNN\n\nQuestion: Which basic neural architecture perform best when combined with BERT?\n\nAnswer: LSTM-CRF\n\nQuestion: Which basic neural architecture perform best when combined with BERT and a multi-granularity architecture?\n\nAnswer: LSTM-CRF+Multi-task\n\nQuestion: Which basic neural architecture perform best when combined with BERT and a multi-granularity architecture?\n\nAnswer: LSTM-CRF+Multi-task\n\nQuestion: Which basic neural architecture perform best when combined with BERT and a multi-granularity", "answers": ["BERT"], "all_classes": null, "length": 1507}
{"pred": "The data was collected using crowdsourcing.\n\nQuestion: what is the size of the database?\n\nAnswer: The database consists of 1969 respondents, 11400 of them are male and 869 are female.\n\n\nQuestion: what is the database is used for text-prompted speaker verification?\n\nAnswer: The database is used for text-prompted speaker verification.\n\n\nQuestion: what is the database is used for text-prompted?\n\nAnswer: The database is used for text-prompted.", "answers": ["Android application"], "all_classes": null, "length": 3795}
{"pred": "We used two machine learning (ML) and deep learning (DL) methods for RQE: Logistic Regression (LR) and Recurrent Neural Networks (RNN) with Long Short Term Memory Cellular Neural Networks (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LSTM) (LST) (LST) (", "answers": ["Logistic Regression, neural networks"], "all_classes": null, "length": 7257}
{"pred": "The benchmark dataset is the Honeypot dataset. It is a public dataset which contains 2218 legitimate users and 2947 spammers. The quality of the dataset is high.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Social Honeypot dataset (public) and Weibo dataset (self-collected); yes", "Social Honeypot, which is not of high quality"], "all_classes": null, "length": 2242}
{"pred": "The decoder has an LSTM encoder and an LSTM decoder.\n\nQuestion: What is the auxiliary task?\n\nAnswer: The auxiliary task is to predict the MSD tag of the target form.\n\nQuestion: What is the effect of the auxiliary task?\n\nAnswer: The effect of the auxiliary task is to regularise the learning process.\n\nQuestion: What is the effect of the multilingual approach?\n\nAnswer: The effect of the multilingual approach is to increase the variance of the results.\n\nQuestion: What is the effect of the", "answers": ["LSTM", "LSTM"], "all_classes": null, "length": 2289}
{"pred": "No.\n\nQuestion: What is the number of events in the dataset?\n\nAnswer: 2,453.\n\nQuestion: What is the number of documents in the dataset?\n\n\nAnswer: 1,1,2,3,4,5,6,7,8,9,10,111,2,3,4,5,6,7,8,9,10,111,2,3,4,5,6,7,8,9,1,2,3,4,5,6,7,8", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3838}
{"pred": "The best performing model is the ensemble of r19 (i.e., r4, r7, r12) on dev (internal) and r19 (i.e., r4, r7, r12) on dev (external). The performance is 0.673 (F1) on dev (internal) and 0.673 (F1) on dev (external).\n\nAnswer: The best performing model is the ensemble of r19 (i.e., r4, r7, r12) on dev (internal) and r19", "answers": ["For SLC task, the \"ltuorp\" team  has the best performing  model (0.6323/0.6028/0.6649 for F1/P/R  respectively) and for FLC task the \"newspeak\" team  has the best performing  model (0.2488/0.2863/0.2201 for F1/P/R respectively)."], "all_classes": null, "length": 1541}
{"pred": "the baseline is the M2M Transformer model trained on the original parallel data.\n\n Question: what is the size of the monolingual data?\n\n Answer: the monolingual data are summarized in Table TABREF8 .\n\n\n Question: what is the size of the parallel data?\n\n Answer: the size of the monolingual data are summarized in Table TABREF8 .\n\n\n Question: what is the size of the of the monoling data?\n\n Answer: the monolingual data are summarized in Table TABREF8", "answers": ["pivot-based translation relying on a helping language BIBREF10, nduction of phrase tables from monolingual data BIBREF14 , attentional RNN-based model (RNMT) BIBREF2, Transformer model BIBREF18, bi-directional model BIBREF11, multi-to-multi (M2M) model BIBREF8, back-translation BIBREF17", "M2M Transformer"], "all_classes": null, "length": 4542}
{"pred": "0.6103\n\nQuestion: What was their highest MRR score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest MRRank score?\n\n\nAnswer: 0.3\n\n\nQuestion: What was their highest MRRank score?\n\nAnswer: 0.3\n\nQuestion: What was their highest MRRank score?\nAnswer: 0.3\n\nQuestion: What was their highest MRRank score?\nAnswer: 0.3\n\nQuestion: What was their highest MRRank score?\n", "answers": ["0.7033", "0.7033"], "all_classes": null, "length": 6810}
{"pred": "The paper explores a number of different embedding techniques, including word embeddings, phrase embeddings, and sentence embeddings.\n\nAnswer: The paper explores a number of different embedding techniques, including word embeddings, phrase embeddings, and sentence embeddings.\n\n\nAnswer: The paper explores a number of different embedding techniques, including word embeddings, phrase embeddings, and sentence embeddings.\n\n\nAnswer: The paper explores a number of different embedding techniques, including word embeddings, phrase embeddings, and sentence embeddings, and related", "answers": ["Skip–gram, CBOW", "integrated vector-res, vector-faith, Skip–gram, CBOW"], "all_classes": null, "length": 4259}
{"pred": "The CFILT-preorder system contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) Hindi-tuned rules that improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.\n\nAnswer: The CFILT-preorder system contains two re-ordering systems: (1) generic rules that apply to all Indian languages and (2) Hindi-tuned rules that improve the generic rules by incorporating improvements found through an error analysis of English-Hindi reordering.\n\n", "answers": ["Unanswerable", "CFILT-preorder system"], "all_classes": null, "length": 2231}
{"pred": "Yes\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from clinical text?\n\n\nAnswer: Yes\n\n\nQuestion: Does the paper explore extraction from biomedical literature?\n\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from clinical text?\nAnswer: Yes\n\nQuestion: Does the paper explore extraction from biomedical text?\nAnswer: Yes\nQuestion: Does the paper explore extraction from clinical text?\nAnswer: Yes\nQuestion: Does the paper", "answers": ["Yes"], "all_classes": null, "length": 3035}
{"pred": "We recruit crowdworkers who have been conferred “master” status and are located within the United States of America. Turkers are asked to imagine they have access to a trusted third-party privacy assistant, to whom they can ask any privacy policy questions.\n\n\nAnswer: We recruit experts with legal training and answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability of the answerability", "answers": ["Individuals with legal training", "Yes"], "all_classes": null, "length": 3846}
{"pred": "The models used for painting embedding are a CNN-RNN based image-to-poem net and a seq2seq model with parallel text corpus for text style transfer.\n\nAnswer: The seq2seq model used is a sequence-to-sequence model with global attention.\n\nAnswer: The seq2seq model used is a sequence-to-sequence model with pointer networks.\n\nAnswer: The seq2seq model used is a sequence-to-sequence model with global attention.\n\nAnswer: The seq2seq model used is a sequence-to-sequence model with pointer networks.\n\nAnswer:", "answers": ["generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models"], "all_classes": null, "length": 1653}
{"pred": "The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\n\nAnswer: The transformer layer works better.\n\n\nAnswer: The transformer layer works better.\n\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer layer works better.\n\nAnswer: The transformer", "answers": ["Transformer over BERT (ToBERT)", "The transformer layer"], "all_classes": null, "length": 2655}
{"pred": "Yes.\n\nQuestion: What is the purpose of the data enrichment method?\n\nAnswer: To extract inter-word semantic connections from each given passage-question pair, which are regarded as the general knowledge we need to extract.\n\n\nQuestion: What is the purpose of the data enrichment method?\n\nAnswer: To extract inter-word semantic connections from each given passage-question pair, which is regarded as the general knowledge we need to extract.\n\n\nQuestion: What is the purpose of the data enrichment method?\n\nAnswer: To extract inter-word semantic connections from each given", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4127}
{"pred": "They addressed the following topics:\n\n1. Cyberbullying on social media\n2. Cyberbullying on social media is a problem\n3. Cyberbullying is a problem\n4. Cyberbullying is a problem\n5. Cyberbullying is a problem\n6. Cyberbullying is a problem\n7. Cyberbullying is a problem\n8. Cyberbullying is a problem\n9. Cyberbullying is a problem\n10. Cyberbullying is a problem\n11. Cyberbullying is a problem\n2. Cyberbul", "answers": ["personal attack, racism, and sexism", "racism, sexism, personal attack, not specifically about any single topic"], "all_classes": null, "length": 3244}
{"pred": "The context representation is obtained by concatenating the left context, the middle context and the right context.\n\nQuestion: How do they train the CNN?\n\nAnswer: The CNN is trained by using the standard convolutional layer with 1200 filters and a softmax layer.\n\nQuestion: How do they train the RNN?\n\nAnswer: The RNN is trained by using a uni-directional RNN which predicts the relation after processing the whole sentence.\n\nQuestion: How do they train the RNN with the ranking layer?\n\nAnswer: The RNN is trained by using", "answers": ["They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation."], "all_classes": null, "length": 2435}
{"pred": "There are 10 different types of entities in the dataset.\n\nQuestion: What is the name of the dataset used in this paper?\n\nAnswer: The dataset used in this paper is called OurNepali dataset.\n\n\nQuestion: What is the name of the dataset used in this paper?\n\nAnswer: The dataset used in this paper is called OurNepali dataset.\n\nQuestion: What is the name of the dataset used in this paper?\n\nAnswer: The dataset is called OurNepali\n\nQuestion: What is the name of the dataset used in this paper?\n\n", "answers": ["OurNepali contains 3 different types of entities, ILPRL contains 4 different types of entities", "three"], "all_classes": null, "length": 2851}
{"pred": "The resulting annotated data is higher quality than the original data.\n\nQuestion: What is the most difficult sentence in the article?\n\nAnswer: The most difficult sentence in the article is \"The study was conducted in the following sentence.\n\n\nQuestion: What is the most difficult sentence in the article?\n\nAnswer: The most difficult sentence is the article.\n\nQuestion: What is the most difficult sentence in the article?\n\nAnswer: The most difficult sentence in the article.\n\n\nQuestion: What is the most difficult sentence in the article?\nAnswer: The most difficult sentence in the article.", "answers": ["improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added"], "all_classes": null, "length": 4399}
{"pred": "The gender representation in the corpora is 65% men and 35% women.\n\nQuestion: How does gender bias vary across speaker's role and speech type?\n\n\nAnswer:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Women represent 33.16% of the speakers"], "all_classes": null, "length": 4055}
{"pred": "The article states that the deliberation models achieve the state of the art results on the English-German dataset.\n\nQuestion: What is the main finding of the article?\n\nAnswer: The main finding of the article is that deliberation models achieve state of the art results on the English-German dataset.\n\nQuestion: What is the main contribution of the article?\n\nAnswer: The main contribution of the article is that deliberation models achieve state of the art results on the English-German dataset.\n\nQuestion: What is the main conclusion of the article?\n\nAnswer: The main conclusion of", "answers": ["the English-German dataset"], "all_classes": null, "length": 1833}
{"pred": "The CWS task is often modelled as one graph model based on an encoder-based scoring model. The model for CWS task is composed of an encoder to represent the input and a decoder based on the encoder to perform segmentation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Baseline models are:\n- Chen et al., 2015a\n- Chen et al., 2015b\n- Liu et al., 2016\n- Cai and Zhao, 2016\n- Cai et al., 2017\n- Zhou et al., 2017\n- Ma et al., 2018\n- Wang et al., 2019"], "all_classes": null, "length": 3629}
{"pred": "The classifiers are deep neural networks.\n\nQuestion: What is the dataset used for training?\n\nAnswer: The dataset is a collection of microblog posts from Twitter.\n\n\nQuestion: What is the dataset used for training?\n\n\nAnswer: The dataset is a collection of microblog posts from Twitter.\n\nQuestion: What is the dataset used for training?\n\nAnswer: The dataset is a collection of microblog posts from Twitter.\n\n\nQuestion: What is the dataset used for training?\n\nAnswer: The dataset is a collection of microblog posts from Twitter.\n\n\nQuestion:", "answers": ["probabilistic model", "Logistic Regression, Multilayer Perceptron"], "all_classes": null, "length": 4475}
{"pred": "\n\nGoogle Cloud\n\nTensiStrength\n\nRosette Text Analytics\n\nCogComp-NLP\n\nStanford NER\n\nTwitterNLP\n\nBIBREF1: BIBREF28\n\nBIBREF2: BIBREF29\n\nBIBREF3: BIBREF30\n\nBIBREF4: BIBREF31\n\nBIBREF5: BIBREF32\n\nBIBREF6: BIBREF33\n\nBIBREF7: BIBREF34\n\nBIB", "answers": ["BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21", "BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26"], "all_classes": null, "length": 1452}
{"pred": "We conduct experiments on the SQuAD dataset BIBREF0.\n\nQuestion: What is the task definition?\n\nAnswer: We formalize our task as an answer-aware Question Generation (QG) task of generating questions from a single-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-to-", "answers": ["SQuAD", "SQuAD"], "all_classes": null, "length": 3757}
{"pred": "\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\n\nAnswer:\n\n\nAnswer:\n\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\n\nAnswer:\nAnswer:\nAnswer:\n\nAnswer:\nAnswer:\n\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:", "answers": ["BOW-Tags, BOW-KL(Tags), BOW-All, GloVe"], "all_classes": null, "length": 4658}
{"pred": "Yes\n\nQuestion: Do they use attention?\n\nAnswer: No\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n\nQuestion: Do they use attention?\n\nAnswer: Unanswerable\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1687}
{"pred": "They used CSAT, 20newsgroups and Fisher datasets.\n\nAnswer the question as concisely as you can, using a single phrase or sentence if possible. Do not provide any explanation.\n\n\nAnswer:\n\n\n\nAnswer the question.\n\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer:\nAnswer", "answers": ["CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus", "CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus"], "all_classes": null, "length": 2652}
{"pred": "\n\nThe article describes a neural sequence model for sentiment classification. The dataset used is the IMDB dataset, which contains movie reviews.\n\nQuestion: What is the model architecture?\n\n\nThe model is a quasi-recurrent neural network (QRNN) model.\n\n\nQuestion: What is the model architecture?\n\n\nThe model is a quasi-recurrent model is a stacked QRNNs, which are the model is a stacked QRNNs, which are the model is a stacked QRNNs, which is a stacked QRNNs, which is a", "answers": ["the IMDb movie review dataset BIBREF17", "IMDb movie review"], "all_classes": null, "length": 3432}
{"pred": "No.\n\nAnswer: unanswerable\n\nAnswer: yes\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 1464}
{"pred": "No.\n\nReason: The dataset is not balanced.\n\nAnswer: No.\n\nReason: The dataset is not balanced.\n\nAnswer: No.\n\nReason: The dataset is not balanced.\n\nAnswer: No.\n\nReason: The dataset is not balanced.\n\nAnswer: No.\n\nReason: The dataset is not balanced.\n\nAnswer: No.\n\nReason: The dataset is not balanced.\n\nAnswer: No.\n\nReason: The dataset is not balanced.\n\nAnswer: No.", "answers": ["No"], "all_classes": null, "length": 1441}
{"pred": "The invertibility condition is a requirement that the projection function INLINEFORM0 is invertible.\n\nQuestion: What is the purpose of the invertibility condition?\n\n\nAnswer: The invertibility condition is used to enable tractable exact inference and exact inference and exact inference and exact inference.\n\nQuestion: What is the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose of the purpose", "answers": ["The neural projector must be invertible.", "we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists"], "all_classes": null, "length": 4323}
{"pred": "\n\nFramework for MRC Gold Standard Analysis ::: Dimensions of Interest\n\nFramework for MRC Gold Standard Analysis ::: Dimensions of Interest ::: Problem setting\n\n\nFramework for MRC Gold Standard Analysis ::::: Dimensions of Interest\n\n\n\nFramework for MRC Gold Standard Analysis ::: Dimensions of Interest\n\n\nFramework for MRC Gold Standard Analysis ::: Dimensions of Interest\n\n\nFramework for MRC Gold Standard Analysis ::: Dimensions of Interest\n\n\nFramework for MRC Gold Standard Analysis ::: Dimensions of Interest\n\n\nFramework for MRC Gold Standard Analysis :::", "answers": ["The resulting taxonomy of the framework is shown in Figure FIGREF10", "FIGREF10"], "all_classes": null, "length": 4958}
{"pred": "The size of WikiSmall is 89,042 sentence pairs, and the size of WikiLarge is 296,402 sentence pairs.\n\nQuestion: what are the metrics used to evaluate the performance of the models?\n\nAnswer: The metrics are BLEU, FKGL, and SARI.\n\nQuestion: what are the results of the models?\n\nAnswer: The results of our method (NMT+synthetic) are better than the baseline NMT system.\n\nQuestion: what are the results of the human evaluation?\n\n", "answers": ["training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing", "WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. "], "all_classes": null, "length": 2266}
{"pred": "The baselines are the cascaded model, the end-to-end model, and the cascaded model with re-segmentation.\n\nQuestion: What are the contributions?\n\n\nAnswer: The contributions are the subnet-level experiments.\n\n\nQuestion: What are the related work?\n\n\nAnswer: The related work is the related work is the related work is the related work is the related work is the related work is the related work is the related work is the related work is the related work is the related work is the related work is the related work is the related work is the related", "answers": ["Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation", "Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train", "Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.\n\nPre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.\n\nMulti-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\\alpha _{st}=0.75$ while $\\alpha _{asr}=0.25$ or $\\alpha _{mt}=0.25$. For many-to-many setting, we use $\\alpha _{st}=0.6, \\alpha _{asr}=0.2$ and $\\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.\n\nMany-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "], "all_classes": null, "length": 4704}
{"pred": "English\n\nQuestion: What is the name of the dataset used in this paper?\n\nAnswer: PTC\n\nQuestion: What is the task?\n\nAnswer: Detecting Propaganda\n\n\nQuestion: What is the question?\n\nAnswer: Sentence-level classification of classification of propaganda, and of the ability to generalise on the task is more likely to occur of the dataset is the minority class is the class is the class is the class is the class is the class is the class is the class is the class is the class is the class is the class is the class is the", "answers": ["Unanswerable", "English"], "all_classes": null, "length": 4513}
{"pred": "The models used in the experiment are a linear SVM, a bidirectional LSTM, and a CNN.\n\nQuestion: What is the performance of the models?\n\nAnswer: The performance of the models is 0.80, 0.69, and 0.69, respectively.\n\nQuestion: What is the performance of the models in the experiment?\n\nAnswer: The performance of the models is 0.80, 0.69, and 0.69, respectively.\n\nQuestion: What is the performance of the models in the experiment?", "answers": ["linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)", "linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) "], "all_classes": null, "length": 2250}
{"pred": "No, the questions are not necessarily useful.\n\nQuestion: Do the answered questions measure for the quality of the answer?\n\nAnswer: No, the questions are not necessarily useful.\n\nQuestion: Do the answered questions measure for the quality of the question?\n\nAnswer: No, the questions are not necessarily useful.\n\nQuestion: Do the answered questions measure for the quality of the answer?\n\nAnswer: No, the questions are not necessarily useful.\n\nQuestion: Do the answered questions measure for the quality of the question?\n\nAnswer: No, the questions are not necessarily useful.\n\nQuestion", "answers": ["No"], "all_classes": null, "length": 1561}
{"pred": "GloVe and Edinburgh embeddings\n\nQuestion: what feature importance graphs were plotted?\n\nAnswer: +/-EffectWordNet, NRC Hashtag Sentiment Lexicon, Sentiment140 Lexicon, NRC Hashtag Emotion Lexicon\n\nQuestion: what is the best way to improve the system?\n\nAnswer: sentence embeddings\n\nQuestion: what is the best way to improve the system?\n\nAnswer: sentence embeddings\n\nQuestion: what is the best way to improve the system?\n\nAnswer: sentence embeddings\n\nQuestion: what", "answers": ["Pretrained word embeddings  were not used", "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16"], "all_classes": null, "length": 1771}
{"pred": "The personalized models outperform the baseline model in BPE perplexity, and generate more diverse and acceptable recipes.\n\nQuestion: How do the personalized models perform in terms of coherence?\n\n\nAnswer: The personalized models perform better than the baseline model in terms of coherence.\n\nQuestion: How do the personalized models perform in terms of coherence?\n\nAnswer: The personalized models perform better than the baseline model in terms of coherence.\n\nQuestion: How do the personalized models perform in terms of coherence?\n\nAnswer", "answers": ["average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time"], "all_classes": null, "length": 2666}
{"pred": "The combination of rewards for reinforcement learning is the combination of the irony reward, the sentiment reward and the content reward.\n\nQuestion: What is the combination of rewards for reinforcement learning?\n\n\nAnswer: The combination of rewards for reinforcement learning is the combination of the irony reward, the sentiment reward, the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination of the combination", "answers": ["irony accuracy, sentiment preservation", " irony accuracy and sentiment preservation"], "all_classes": null, "length": 4592}
{"pred": "The authors demonstrate that the model does not work well in practice when the style transfer dataset does not have similar words in the training set of sentences.\n\nQuestion: What is the main contribution of the paper?\n\nAnswer: The main contribution of the paper is that the authors demonstrate that a CNN-RNN based image-to-poem net combined with a seq2seq model with parallel text corpus for text style transfer synthesizes Shakespeare-style prose for a given painting.\n\nQuestion: What is the main limitation of the paper?\n\nAnswer: The main limitation of the paper is that the authors do", "answers": ["Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer", "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score"], "all_classes": null, "length": 1651}
{"pred": "They compared to existing methods, including those based on hand-crafted resources, and those based on unsupervised learning.\n\nQuestion: What is the name of the dataset they used for evaluation?\n\n\nAnswer: The Affective Text dataset.\n\n\nQuestion: What is the name of the dataset they used for the evaluation?\n\nAnswer: The Affective Text dataset.\n\nQuestion: What is the name of the dataset they used for evaluation?\nAnswer: The Fairy\n\n\nQuestion: What is the name of the dataset they used for evaluation?\nAnswer: The Fairy\n", "answers": ["Affective Text, Fairy Tales, ISEAR", " Affective Text dataset, Fairy Tales dataset, ISEAR dataset"], "all_classes": null, "length": 3390}
{"pred": "The article states that viral tweets containing fake news were created more recently than other viral tweets. However, the number of retweets, favourites, hashtags, and mentions were not statistically different between viral tweets containing fake news and those not containing fake news.\n\n\nAnswer: The article states that viral tweets containing fake news were created more recently than other viral tweets. However, the number of retweets containing fake news and the number of favourites, and mentions were not statistically different between viral tweets and those not containing fake news.\n\n\n\n", "answers": ["Distributions of Followers, Friends and URLs are significantly different between the set of tweets containing fake news and those non containing them, but for Favourites, Mentions, Media, Retweets and Hashtags they are not significantly different"], "all_classes": null, "length": 3164}
{"pred": "The dataset is sourced from Twitter.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset contains 12,594 hashtags and their corresponding tweet.\n\n\nQuestion: What is the hashtag segmentation for each hashtag.\n\nAnswer: The hashtag segmentation for each hashtag.\n\nQuestion: What is the hashtag segmentation method used?\n\nAnswer: The hashtag segmentation method is used?\n\nQuestion: What is the hashtag segmentation method used?\nAnswer: The has", "answers": ["1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset", "Stanford Sentiment Analysis Dataset BIBREF36"], "all_classes": null, "length": 3756}
{"pred": "The corpus contains Persian accents.\n\nQuestion: what is the number of speakers in the corpus?\n\nAnswer: The corpus contains 1969 speakers.\n\n\nQuestion: what is the number of speakers in the question.\n\nAnswer: The number of speakers is 1969.\n\nQuestion: what is the number of speakers in the corpus?\n\nAnswer: The corpus contains 1969.\n\nQuestion: what is the number of speakers in the corpus?\nAnswer: The corpus contains 1", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 3794}
{"pred": "Word subspace can represent the context of a text.\n\nQuestion: What is the main idea of the word subspace modeling?\n\nAnswer: The main idea of the word subspace modeling is to represent the context of the word subspace of the word subspace.\n\nQuestion: What is the main idea of the word subspace modeling?\n\nAnswer: The main idea of the word subspace modeling is to represent the context of the word subspace.\n\nQuestion: What is the main idea of the word subspace modeling?\n\nAnswer: The main idea of the word sub", "answers": ["Word vectors, usually in the context of others within the same class"], "all_classes": null, "length": 5151}
{"pred": "Random Forests (RF)\n\nQuestion: What is the performance of the baseline model?\n\nAnswer: Precision=0.12\n\nQuestion: What is the performance of the ASP task?\n\n\nAnswer: We use the ASP task.\n\nQuestion: What is the performance of the ASP task?\nAnswer: The performance of the ASP task is the ASP task.\nQuestion: What is the performance of the ASP task?\nAnswer: The performance of the ASP task.\nQuestion: What is the performance of the ASP task?\nAnswer: The performance of the ASP task.", "answers": ["For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. \n\nFor Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section.", "B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .\n\n, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"], "all_classes": null, "length": 7891}
{"pred": "No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 2000}
{"pred": "The LibriSpeech dataset is a large-scale speech corpus that contains over 100 hours of English speeches. The dataset is divided into training, development, and test sets. The training set contains 10000000000000000000000000000000000000000000000000000000000000000000000000000000000", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2410}
{"pred": "The dataset for fine-grained classification is split in train, development, development_test, test parts.\n\nQuestion: What is the primary measure of performance?\n\nAnswer: The primary measure of performance is the macro-averaged Mean Absolute Error (INLINEFORM0 ) given by the embeddings of the words that compose a tweets.\n\nQuestion: What is the secondary measure of performance?\n\nAnswer: The secondary measure is the micro-averaged by the performance is the micro-averaged by the performance is the micro-averaged performance is the micro-averaged performance is the micro", "answers": [" high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task", " SemEval-2016 “Sentiment Analysis in Twitter”"], "all_classes": null, "length": 2738}
{"pred": "They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\nAnswer: They use large BERT.\n\n", "answers": ["small BERT", "small BERT"], "all_classes": null, "length": 1999}
{"pred": "Yes, the automatically constructed datasets are subject to quality control. The quality control is done by human experts who are experts in the field of science. The human experts are asked to answer the questions in the article. The human experts are asked to answer the question.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["No", "No"], "all_classes": null, "length": 6391}
{"pred": "Yes\n\nQuestion: Are the images from a specific domain?\n\nAnswer: No\n\nQuestion: Are the images from a specific domain?\n\nAnswer: Unanswerable\n\n\nQuestion: Are the images from a specific domain?\n\n\nAnswer: Yes\n\n\nQuestion: Are the images from a specific domain?\n\nAnswer: No\n\nQuestion: Are the images from a specific domain?\n\nAnswer: No\n\n\nQuestion: Are the images from a specific domain?\nAnswer: Unanswerable\n\n\nQuestion: Are the images from a specific domain?\nAnswer: No\n", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3472}
{"pred": "The best model (B-M) on development data relies entirely on automatically obtained information, both in terms of training data as well as features.\n\nQuestion: What is the best model?\n\n\nAnswer: The best model is B-M.\n\n\nQuestion: What is the best model?\n\nAnswer: The best model is the best model.\n\nQuestion: What is the best model?\n\nAnswer: The best model.\n\n\nQuestion: What is the best model?\nAnswer: The best model.\n\nQuestion: What is the best model?\nAnswer: The best model", "answers": ["Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. "], "all_classes": null, "length": 3410}
{"pred": "The tagging scheme employed is the INLINEFORM0 tagging scheme.\n\nQuestion: What is the input to the model?\n\nAnswer: The input to the model is the character-level features, word-level features, and positional features.\n\n\nQuestion: What is the output of the model?\n\nAnswer: The output of the model is the output is the model is the output is the output is the output is the output is the output is the output is the output is the output is the output is the output is the output is the output is the output is the output is the output is the", "answers": ["A new tagging scheme that tags the words before and after the pun as well as the pun words.", "a new tagging scheme consisting of three tags, namely { INLINEFORM0 }"], "all_classes": null, "length": 2974}
{"pred": "No.\n\nQuestion: Is CoVost a multilingual speech-to-text translation corpus?\n\nAnswer: Yes.\n\nQuestion: Is CoVost a many-to-one multilingual speech-to-one multilingual many-to-one multilingual many-to-one multilingual many-one multilingual many-to-one multilingual many-to-one multilingual many-to-one multilingual many-to-one multilingual many-to-one multilingual many-to-one multilingual many-to-", "answers": ["No", "No"], "all_classes": null, "length": 2413}
{"pred": "Robustness is defined as the ability of a model to perform well in the presence of noise.\n\nAnswer: Robustness is defined as the ability of a model to perform well in the presence of noise.\n\n\nAnswer: How toxicology is defined as the ability of a model to perform well in the presence of noise.\n\nAnswer: Robustness is defined as the ability of a model to perform well in the presence of noise.\n\n\nAnswer: Robustness is defined as the ability of a model to perform well in the presence of noise.\n\nAnswer: Robustness is defined", "answers": ["ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced", "Low sensitivity to bias in prior knowledge"], "all_classes": null, "length": 3609}
{"pred": "We compared SBERT to average GloVe embeddings, InferSent BIBREF4, and Universal Sentence Encoder BIBREF5.\n\nQuestion: What is the computational efficiency of SBERT is faster than InferSent is about 9% faster than InferSent and about 5555% faster than Universal Sentence Encoder BIBREF5.\n\nAnswer: SBERT is about 9% faster than InferSent is about 5% faster than Universal Sentence Encoder BIBREF5.\n\n\nQuestion: What is the", "answers": ["GloVe, BERT, Universal Sentence Encoder, TF-IDF, InferSent", "Avg. GloVe embeddings, Avg. fast-text embeddings, Avg. BERT embeddings, BERT CLS-vector, InferSent - GloVe and Universal Sentence Encoder."], "all_classes": null, "length": 3862}
{"pred": "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively. We observe huge performance boosts on all of the four NER datasets.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively", "For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively"], "all_classes": null, "length": 3586}
{"pred": "They test their conflict method on two tasks:\n\n1. Pair-classification task on Quora duplicate question pairs.\n\n2. Ranking questions in Bing's People Also Ask.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions", "Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask"], "all_classes": null, "length": 2577}
{"pred": "They compared against the following baselines:\n\n- Bidirectional LSTM (BiLSTM)\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiLSTM with generalized pooling\n- BiL", "answers": ["Various tree structured neural networks including variants of Tree-LSTM, Tree-based CNN, RNTN, and non-tree models including variants of LSTMs, CNNs, residual, and self-attention based networks", "Sentence classification baselines: RNTN (Socher et al. 2013), AdaMC-RNTN (Dong et al. 2014), TE-RNTN (Qian et al. 2015), TBCNN (Mou et al. 2015), Tree-LSTM (Tai, Socher, and Manning 2015), AdaHT-LSTM-CM (Liu, Qiu, and Huang 2017), DC-TreeLSTM (Liu, Qiu, and Huang 2017), TE-LSTM (Huang, Qian, and Zhu 2017), BiConTree (Teng and Zhang 2017), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), TreeNet (Cheng et al. 2018), CNN (Kim 2014), AdaSent (Zhao, Lu, and Poupart 2015), LSTM-CNN (Zhou et al. 2016), byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017), BCN + Char + CoVe (McCann et al. 2017), BCN + Char + ELMo (Peters et al. 2018). \nStanford Natural Language Inference baselines: Latent Syntax Tree-LSTM (Yogatama et al. 2017), Tree-based CNN (Mou et al. 2016), Gumbel Tree-LSTM (Choi, Yoo, and Lee 2018), NSE (Munkhdalai and Yu 2017), Reinforced Self- Attention Network (Shen et al. 2018), Residual stacked encoders: (Nie and Bansal 2017), BiLSTM with generalized pooling (Chen, Ling, and Zhu 2018)."], "all_classes": null, "length": 4781}
{"pred": "The core component for KBQA is the relation detection model.\n\nQuestion: What is the core component for KBQA?\n\nAnswer: The core component for KBQA is the relation detection model.\n\n\n\n\n\n\nQuestion: What is the core component for KBQA?\n\n\nAnswer: The core component for KBQA is the core component for KBQA is the core component for KBQA is the core component for KBQA is the core component for KBQA is the core component for KBQA is the core", "answers": ["answer questions by obtaining information from KB tuples ", "hierarchical matching between questions and relations with residual learning"], "all_classes": null, "length": 4527}
{"pred": "The baseline models are the encoder-decoder model and the encoder-decoder model with attention.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personalized attention model.\n\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personalized models.\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personalized models.\n\n\nQuestion: What are the personalized models?\n\nAnswer: The personalized models are the personalized models.\n\n", "answers": ["name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)"], "all_classes": null, "length": 2655}
{"pred": "The methods to find examples of biases and unwarranted inferences are manual inspection of the data, and tagging the data with part-of-speech information.\n\nAnswer: The methods to find examples of biases and unwarranted inferences are manual inspection of the data, and tagging the data with part-of-speech information.\n\nAnswer: The methods to find examples of biases and unwarranted inferences are manual inspection of the data, and tagging the data with part-of-speech information.\n\nAnswer: The methods to find examples of biases", "answers": ["spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering", "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging"], "all_classes": null, "length": 2204}
{"pred": "English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\nAnswer: English\n\n", "answers": ["English, French, German ", "French, English, Spanish, Italian, Portuguese, Hebrew, Arabic"], "all_classes": null, "length": 2285}
{"pred": "They experimented with the following models:\n\n1. CAS-LSTMs with different forget gates\n2. CAS-LSTMs with different forget gates\n3. CAS-LSTMs without forget gates\n4. CAS-LSTMs with different forget gates\n5. CAS-LSTMs with different forget gates\n6. CAS-LSTMs with different forget gates\n7. CAS-LSTMs with different forget gates\n8. CAS-LSTMs with different forget gates\n9. CAS-LST", "answers": ["Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers"], "all_classes": null, "length": 3210}
{"pred": "No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\n\nAnswer: No.\n\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\nAnswer: No.\n\nQuestion: Do they report results only on English data?\n\nAnswer: No.\n\nQuestion: Do they report results only on English data?\nAnswer: No.\n\nQuestion: Do they report only on English data?", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 6169}
{"pred": "The authors experimented with several sentence classification algorithms to identify strengths, weaknesses and suggestions for improvements found in the supervisor assessments. They also used clustering to discover broad categories among them. As this is non-topical classification, they found that SVM with ADWS kernel produced the best results.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["LSA, TextRank, LexRank and ILP-based summary.", "LSA, TextRank, LexRank"], "all_classes": null, "length": 3045}
{"pred": "The previous state of the art for this task is to use a neural network to predict whether an instructor will intervene in a thread.\n\nAnswer: The previous state of the art for this task is to use a neural network to predict whether an instructor will intervene in a thread.\n\n\nAnswer: The previous state of the art for this task is to use a neural network to predict whether an instructor will intervene in a thread.\n\n\nAnswer: The previous state of the art for this task is to use a neural network to predict whether an instructor will intervene in a thread.\n\n", "answers": ["hLSTM", "hLSTM"], "all_classes": null, "length": 3725}
{"pred": "The Message Passing Attention network for Document understanding (MPAD)\n\nAnswer: The Message Passing Attention network for Document understanding (MPAD)\n\nAnswer: The Message Passing Attention network for Document understanding (MPAD)\n\n\n\nAnswer: The Message Passing Attention network for Document understanding (MPAD)\n\nAnswer: The Message Passing Attention network for Document understanding (MPAD)\n\n\nAnswer: Which component is the most impactful?\n\n\nAnswer: The Message Passing Attention network for Document understanding (MPAD)\n\nAnswer: Which component is the", "answers": ["Based on table results provided changing directed to undirected edges had least impact - max abs difference of 0.33 points on all three datasets."], "all_classes": null, "length": 4180}
{"pred": "The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus BIBREF11.\n\nAnswer: The corpus used for the task is the DTA corpus", "answers": ["DTA18, DTA19", "Diachronic Usage Relatedness (DURel) gold standard data set"], "all_classes": null, "length": 1908}
{"pred": "Kannada, Hindi, Telugu, Malayalam, Bengali, and English.\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n\nAnswer: unanswerable\n", "answers": ["Hindi, English, Kannada, Telugu, Assamese, Bengali and Malayalam", "Kannada, Hindi, Telugu, Malayalam, Bengali,  English and Assamese (in table, missing in text)"], "all_classes": null, "length": 2453}
{"pred": "The model performance on target language reading comprehension is shown in Table TABREF32.\n\n Question: What does zero-shot transfer model learn?\n\nAnswer: The model learns to find answers by matching words in contexts or questions with their synonyms in another language.\n\n Question: What does zero-shot transfer model learn?\n\nAnswer: The model learns to find answers by matching words in contexts or questions with their synonyms in another language.\n\n Question: What does zero-shot transfer model learn?\n\nAnswer: The model learns to find answers by matching words", "answers": ["Table TABREF6, Table TABREF8", "when testing on English, the F1 score of the model training on Chinese (Zh) is 53.8,  F1 score is only 44.1 for the model training on Zh-En"], "all_classes": null, "length": 2492}
{"pred": "The proposed model outperforms the baselines.\n\nAnswer: The proposed model outperforms the baselines.\n\nAnswer: The proposed model outperforms the baselines.\n\n\nAnswer: The proposed model outperforms the baselines.\n\n\nAnswer: The proposed model outperforms the baselines.\n\n\nAnswer: The proposed model outperforms the baselines.\n\n\nAnswer: The proposed model outperforms the baselines.\n\n\nAnswer: The proposed model outperforms the baselines.\n\n\nAnswer: The proposed model outperforms the baselines.", "answers": ["Metric difference between Aloha and best baseline score:\nHits@1/20: +0.061 (0.3642 vs 0.3032)\nMRR: +0.0572(0.5114 vs 0.4542)\nF1: -0.0484 (0.3901 vs 0.4385)\nBLEU: +0.0474 (0.2867 vs 0.2393)"], "all_classes": null, "length": 5151}
{"pred": "ARAML outperforms other baselines in terms of forward/reverse perplexity and Self-BLEU.\n\nQuestion: What is the difference between RAML and IRL?\n\n\nAnswer: RAML is a reward augmented MLEARNED.\n\n\nQuestion: What is the difference between RAML and IRL?\n\nAnswer: RAML is a rewarded MLEARNED.\n\n\nQuestion: What is the difference between RAML and IRL?\n\nAnswer: RAML is a rewarded.\n\n\nQuestion: What is the difference between RAML and", "answers": ["ARAM has achieved improvement over all baseline methods using reverese perplexity and  slef-BLEU metric.  The maximum reverse perplexity improvement 936,16 is gained for EMNLP2017  WMT  dataset and  48,44 for COCO dataset.", "Compared to the baselines, ARAML does not do better in terms of perplexity on COCO and EMNLP 2017 WMT datasets, but it does by up to 0.27 Self-BLEU points on COCO and 0.35 Self-BLEU on EMNLP 2017 WMT. In terms of Grammaticality and Relevance, it scores better than the baselines on up to 75.5% and 73% of the cases respectively."], "all_classes": null, "length": 3796}
{"pred": "The authors present evidence that the model can capture some biases in data annotation and collection by examining the results of the model. The authors found that the model was able to detect some biases in the process of collecting or annotating datasets by investigating a mixture of contextual information embedded in the BERT’s layers and a set of features associated to the different type of biases in data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate"], "all_classes": null, "length": 4119}
{"pred": "Yes, we also test a number of baselines, including a BERT model without answerability information, a BERT model with answerability information, and a BERT model with answerability information and a model without answerability information.\n\n\n\n\n\n\n\nQuestion: What is the best performing model?\n\n\nAnswer: Bert + answerability information outperforms the best performing model.\n\n\nQuestion: What is the best performing model?\n\n\nAnswer: Bert + answerability information\n\n\nQuestion: What is the best performing model?\nAnswer: Bert + answerability information\n\n\nQuestion", "answers": ["SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance", "No-Answer Baseline (NA), Word Count Baseline, Human Performance"], "all_classes": null, "length": 3855}
{"pred": "The dataset is 100000 words.\n\nQuestion: What is the size of the dataset?\n\nAnswer: The dataset is 10000000 words.\n\n\nQuestion: What is the size of the dataset is 10000000 words.\n\nAnswer: The dataset is 100000000000000000000000000000000000000000000000000000", "answers": ["Dataset contains 3606 total sentences and 79087 total entities.", "ILPRL contains 548 sentences, OurNepali contains 3606 sentences"], "all_classes": null, "length": 2843}
{"pred": "We find that replacing the training objective with dice loss introduces performance boost for both BERT and XLNet. Using dice loss improves the F1 score by +0.58 for MRPC and +0.73 for QQQQQ.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP", "+0.58"], "all_classes": null, "length": 3566}
{"pred": "The data used in this work is from BIBREF0 . The data is available at https://github.com/james-mccoy/language-model-predict-erp.\n\nQuestion: What is the neural network architecture?\n\nAnswer: The neural network architecture is a bi-LSTM encoder followed by a bi-LSTM decoder. The encoder is trained on the language modeling objective, and the decoder is trained on the ERP prediction objective.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure is to train the encoder on", "answers": ["Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.\nSelect:\n- ERP data collected and computed by Frank et al. (2015)\n- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)", "the ERP data: BIBREF0"], "all_classes": null, "length": 1971}
{"pred": "The subjects were presented with a series of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of 11-trial blocks of", "answers": ["7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)", "KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)"], "all_classes": null, "length": 2379}
{"pred": "We use the following baselines for evaluation:\n\n1. Pointer-Gen+Same-FT: The Pointer-Gen model is trained on the same dataset as the sensationalism scorer.\n\n2. Pointer-Gen+Same-FT: The Pointer-Gen model is trained on the same dataset as the sensationalism scorer.\n\n3. Pointer-Gen+Pos-FT: The Pointer-Gen model is trained on the same dataset as the sensationalism scorer.\n\n\n4. Pointer-Gen+Same-FT: The Pointer", "answers": ["Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN"], "all_classes": null, "length": 4085}
{"pred": "The most frequently used learning models are traditional machine learning classifiers such as Naïve Bayes, Logistic Regression, Support Vector Machine, and Random Forests. Neural network models such as Convolutional Neural Networks, Recurrent Neural Networks, and their variants are also used.\n\nQuestion: What are the results of the learning models?\n\nAnswer: The results of the learning models are presented in Table . The most accurate models are neural network models such as RNN with LTC modules, and CNN with context tweets.\n\nQuestion: What are the features of the dataset?\n\n", "answers": ["Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)", "Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN"], "all_classes": null, "length": 2074}
{"pred": "We use the Big Transformer architecture (BIBREF16) and the fairseq toolkit (BIBREF22).\n\nQuestion: What is the vocabulary size of the language model?\n\nAnswer: The vocabulary size of the language model is 32K.\n\nQuestion: What is the vocabulary size of the sequence to sequence model?\n\nAnswer: The vocabulary size of the sequence to sequence model is 32K.\n\nQuestion: What is the number of parameters of the language model?\n\nAnswer: The number of parameters", "answers": ["uni-directional model to augment the decoder", "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder"], "all_classes": null, "length": 1914}
{"pred": "Weights are dynamically adjusted based on the Sørensen–Dice coefficient.\n\nQuestion: What is the Sørensen–Dice coefficient?\n\nAnswer: The Sørensen–dice coefficient is a measure of the similarity between two distributions.\n\n\nQuestion: What is the Tversky coefficient?\n\nAnswer: The Tversky coefficient is the Tversky coefficient is the Tversky coefficient is the Tversky coefficient is the Tversky coefficient is the Tversky coefficient is the Tversky coefficient is the Tversky coefficient is the Tversky coefficient is the Tver", "answers": ["One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.", "associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds"], "all_classes": null, "length": 3640}
{"pred": "The results from these proposed strategies are that the knowledge graph based exploration methods are able to pass the bottleneck in Zork1. The KG-A2C-chained method is able to pass the bottleneck in Zork1 with a score of 40, whereas the Go-Explore based method is able to pass the bottleneck with a score of 40.\n\nAnswer: The results from these proposed strategies are that the knowledge graph based exploration methods are able to pass the bottleneck in Zork1. The KG-A2", "answers": ["Reward of 11.8 for the A2C-chained model, 41.8 for the KG-A2C-chained model, 40 for A2C-Explore and 44 for KG-A2C-Explore.", "KG-A2C-chained and KG-A2C-Explore both pass the bottleneck of a score of 40"], "all_classes": null, "length": 2443}
{"pred": "An individual model consists of a set of parameters that are estimated from the data.\n\nQuestion: What is the monolingual model?\n\nAnswer: The monolingual model is a Bayesian model for unsupervised SRL BIBREF11 .\n\n\nQuestion: What is the multilingual model?\n\nAnswer: A Bayesian model for multilingual model is a joint model for the monolingual model is a model for multiling model is a model for the multilingual model is a model for the multilingual model is a model for the multilingual model is", "answers": ["Bayesian model of garg2012unsupervised as our base monolingual model"], "all_classes": null, "length": 3701}
{"pred": "Non-standard pronunciation is identified by the transcriber.\n\nQuestion: What is the purpose of the resource?\n\nAnswer: The purpose of the resource is to provide a resource for building language systems in Mapudungun.\n\n\nQuestion: What is the resource?\n\nAnswer: The purpose of the resource is to provide a resource.\n\nQuestion: What is the resource?\nAnswer: The resource is to provide a resource.\n\nQuestion: What is the resource?\nAnswer: The resource is the resource.\n\nQuestion: What is the resource?\nAnswer: The resource.", "answers": ["Unanswerable", "Original transcription was labeled with additional labels in [] brackets with nonstandard pronunciation."], "all_classes": null, "length": 3018}
{"pred": "A semicharacter architecture is a type of neural network that processes words with misspelled characters, predicting the correct words at each step.\n\nQuestion: What is a word-only input format?\n\n\nAnswer: A word-only format is a sequence of constituent words with a one hot vector of the first character ( $w_i$ and the last character ( $w_i$ and a bag of characters representation of the context and orthography of the input.\n\n\nQuestion: What is a word recognition model?\n\nAnswer: A word recognition models are trained separately.\n\nQuestion", "answers": ["A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters", "processes a sentence of words with misspelled characters, predicting the correct words at each step"], "all_classes": null, "length": 4186}
{"pred": "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, Swedish\n\n\nQuestion: which languages are covered by the original article?\n\n\nAnswer: Bulgarian, Croatian, Czech, Danish, English, French, German, Italian, Indonesian, Norwegian, Persian, Polish, Portuguese, Spanish, Slovenian, Spanish, Swedish\n\n\nQuestion: which languages are not covered by the original article?\n\nAnswer: Bulgarian, Croatian, Czech, Danish,", "answers": ["Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish", "Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish"], "all_classes": null, "length": 2697}
{"pred": "NCEL outperforms the state-of-the-art collective methods across five different datasets.\n\nQuestion: What are the key modules of NCEL?\n\n\nAnswer: NCEL is a neural collective inference model that combines the local and global information.\n\n\nQuestion: What is the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the impact of the", "answers": ["NCEL consistently outperforms various baselines with a favorable generalization ability"], "all_classes": null, "length": 4113}
{"pred": "Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\n\nAnswer: Yes\n\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\n\nAnswer: Yes\n\nQuestion: Is the data de-identified?\nAnswer: Yes\nQuestion: Is the data de", "answers": ["Yes", "Yes"], "all_classes": null, "length": 4592}
{"pred": "The baseline used was the error detection model by Rei2016.\n\nQuestion: What is the error generation method?\n\nAnswer: The error generation method is a pattern-based method.\n\nQuestion: What is the error generation method?\n\nAnswer: The error generation method is a machine translation method.\n\nQuestion: What is the error generation method?\n\nAnswer: The error generation method is a combination of the pattern-based method and the machine translation method.\n\nQuestion: What is the error generation method?\n\nAnswer: The error generation method is a combination of the pattern-", "answers": ["error detection system by Rei2016", "error detection system by Rei2016"], "all_classes": null, "length": 2132}
{"pred": "The annotated clinical notes were obtained from the i2b2/VA challenge.\n\nQuestion: what is the purpose of the BiLSTM-CRF model?\n\nAnswer: The purpose of the BiLSTM-CRF model is to tag the clinical entities in the user query.\n\n\nQuestion: what is the purpose of the BiLSTM-CRF model is to tag the BiLSTM-CRF model is to tag the purpose of the BiLSTM-CRF model is to tag the purpose of the BiLSTM-CRF model is to", "answers": ["clinical notes from the CE task in 2010 i2b2/VA", "clinical notes from the CE task in 2010 i2b2/VA "], "all_classes": null, "length": 3432}
{"pred": "Masking words in the decoder is helpful because it forces the model to generate more diverse outputs. By masking words, the model is forced to generate more diverse outputs, which can help the model to generate more diverse outputs, which can help the model to generate more diverse outputs, which can help the model to generate more diverse outputs, which can help the model to generate more diverse outputs, which can help the model to generate more diverse outputs, which can help the model to generate more diverse outputs, which can help the model to generate more diverse outputs, which can help the model to generate more diverse outputs, which can help the", "answers": ["ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences."], "all_classes": null, "length": 3919}
{"pred": "The authors use the Twitter dataset BIBREF28 .\n\nQuestion: What is the objective function they optimize?\n\nAnswer: The objective function they optimize is to compute the semantic similarity between two tweets.\n\nQuestion: What is the model architecture?\n\nAnswer: The model architecture is a CNN-LSTM encoder-decoder architecture.\n\nQuestion: What is the training procedure?\n\nAnswer: The training procedure is to train the model on the training set and then evaluate it on the test set.\n\nQuestion: What is the evaluation procedure?\n\nAnswer: The evaluation procedure is", "answers": ["Unanswerable", " Paraphrase Database (PPDB) ,  book corpus", "Unanswerable"], "all_classes": null, "length": 1902}
{"pred": "The features used are TF-IDF features.\n\nQuestion: What is the performance of the model?\n\nAnswer: The performance of the model is 92% accuracy.\n\nQuestion: What is the performance of the model?\n\nAnswer: The performance of the model is 92% accuracy.\n\nQuestion: What is the performance of the model?\n\nAnswer: The performance of the model is 92% accuracy.\n\nQuestion: What is the performance of the model?\n\nAnswer: The performance of the model is 92% accuracy.\n\nQuestion: What", "answers": ["Unanswerable"], "all_classes": null, "length": 2108}
{"pred": "The dataset is annotated with 7 feature groups with associated feature values binarized (i.e., present=1 or absent=0) to represent potentially informative features for classifying Twitter tweets in the depression schema hierarchy.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is a collection of 9,473 annotated Twitter tweets.\n\nQuestion: What is the feature ablation study?\n\nAnswer: The feature ablation study is conducted to assess the contribution of each feature group by quantifying the change in predictive power when comparing the performance of a classifier", "answers": ["no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy", "The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression"], "all_classes": null, "length": 1947}
{"pred": "\n\nThe eight NER tasks were:\n\n1. Biomedical NER\n\n2. Clinical NER\n\n3. ClinicalXL NER\n\n\n4. Clinical NER\n\n\n\n\n\n\n5. Covid-19 NER\n\n6. Covid-19 NER\n\n7. Covid-19 NER\n\n8. Covid-19 NER\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800", "BC5CDR-disease, NCBI-disease, BC5CDR-chem, BC4CHEMD, BC2GM, JNLPBA, LINNAEUS, Species-800"], "all_classes": null, "length": 2800}
{"pred": "The training data was translated by using the machine translation platform Apertium.\n\nQuestion: How was the data augmented?\n\nAnswer: The training data was augmented by using the machine translation platform Apertium.\n\nQuestion: How was the data ensembling performed?\n\nAnswer: The data was ensembling performed by using the machine translation platform Apertium.\n\nQuestion: How was the data ensembling performed?\n\nAnswer: The data was ensembling performed by using the machine translation platform Apertium.\n\nQuestion: How was the data ensembling performed?\n", "answers": ["using the machine translation platform Apertium ", "machine translation platform Apertium BIBREF5"], "all_classes": null, "length": 2423}
{"pred": "They used a content-based classifier.\n\nQuestion: What is the name of the system?\n\nAnswer: The name of the system is \"Text-based classifier for predicting industry from social media users' industry\".\n\n\nQuestion: What is the name of the system?\n\nAnswer: The name of the system is \"Text-based classifier for their system\".\n\nQuestion: What is the name of the system?\n\nAnswer: The name of the system\".\n\n\nQuestion: What is the name of the system?\nAnswer: The name of the system is \"Text-", "answers": ["AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier"], "all_classes": null, "length": 4177}
{"pred": "The baseline is the average of the F$_1$ scores of the 1000 articles in the training set.\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level classification?\n\n\nAnswer: The sentence-level classification is more difficult than the fragment-level classification.\n\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level classification?\n\n\nAnswer: The sentence-level classification is more difficult than fragment-level classification.\n\n\nQuestion: What is the difference between the sentence-level classification and the fragment-level", "answers": ["The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.", "SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly"], "all_classes": null, "length": 3001}
{"pred": "The baselines are the systems that do not adopt joint learning.\n\nQuestion: What is the dataset?\n\nAnswer: The dataset is the Pun Detection and Pun Location datasets.\n\n\nQuestion: What is the INLINEFORMATTACK: What is the INLINEFORMATTACK: What is the INLINEFORMATTACK: What is the INLINEFORMATTACK: What is the INLINEFORMATTACK: What is the INLINEFORMATTACK: What is the INLINEFORMATTACK: What is the INLINEFORMATTACK: What is", "answers": ["They compare with the following models: by Pedersen (2017), by Pramanick and Das (2017), by Mikhalkova and Karyakin (2017),  by Vadehra (2017), Indurthi and Oota (2017), by Vechtomova (2017), by (Cai et al., 2018), and CRF."], "all_classes": null, "length": 2991}
{"pred": "We do not include any political bias in the model. We only consider the source of the news article, which is a binary variable.\n\n Question: How do you determine the size of the network?\n\n\nAnswer: We consider the number of tweets that contain the URL of the article.\n\n\n Question: How do you determine the size of the article?\n\nAnswer: We consider the number of tweets that contain the URL of the article.\n\n\n Question: How do you determine the size of the size of the article?\n\nAnswer: We consider the number of the tweets that contain the", "answers": ["By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains", "we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries."], "all_classes": null, "length": 4882}
{"pred": "The ancient Chinese dataset comes from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of which are designed for Ancient-Modern Chinese alignment.\n\n\n\n\n\n\n\n Answer: The ancient Chinese is a way of the ancient Chinese is a way of the ancient Chinese is a way of the ancient Chinese is a way of the ancient Chinese is a way of the ancient Chinese is a way of the ancient Chinese is a way of the ancient Chinese is a way of the ancient Chinese is a way of the ancient Chinese is a way of", "answers": ["ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era", "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet "], "all_classes": null, "length": 3722}
{"pred": "English\n\nQuestion: Is the tweet offensive?\n\nAnswer: Yes\n\nQuestion: Is the tweet an insult?\n\nAnswer: Yes\n\nQuestion: Is the tweet a threat?\n\nAnswer: Yes\n\nQuestion: Is the tweet targeted at an individual?\n\nAnswer: Yes\n\nQuestion: Is the tweet targeted at a group?\n\nAnswer: Yes\n\nQuestion: Is the tweet targeted at others?\n\nAnswer: Yes\n\nQuestion: Is the tweet targeted at an organization?\n\nAnswer: Yes\n\nQuestion", "answers": ["English", "English ", "English"], "all_classes": null, "length": 2240}
{"pred": "\n\nThe Chinese part of the Penn Treebank (PTB) was used for training and testing. The PTB is a 100M-word corpus of English and Chinese text, and is available at http://www.google.com/url?q=http://www.com/url?q=http://www.com/url?q=http://www.com/url?q=http://www.com/url?q=http://www.com/url?q=http://www.com/url?q=http://www.com/url?q", "answers": ["Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)"], "all_classes": null, "length": 2545}
{"pred": "The UTCNN model has three layers.\n\nAnswer: The UTCNN model has three layers.\n\nAnswer: The UTCNN model has three layers.\n\nAnswer: The UTCNN model has three layers.\n\n\nAnswer: The UTCNN model has three layers.\n\n\nAnswer: The UTCNN model has three layers.\n\nAnswer: The UTCNN model has three layers.\n\nAnswer: The UTCNN model has three layers.\n\n\nAnswer: The UTCNN model has three layers.\n\nAnswer: The UTCNN model has three layers.\n\nAnswer: The UTCNN model has", "answers": ["eight layers"], "all_classes": null, "length": 4487}
{"pred": "The dataset used in this paper is the European Nature Protected Sites dataset.\n\nQuestion: what is the purpose of this paper?\n\nAnswer: The purpose of this paper is to show that the information from Flickr tags can be used to improve the useful as a more effective way.\n\nQuestion: what is the paper?\nAnswer: the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the paper is the", "answers": [" the same datasets as BIBREF7", "same datasets as BIBREF7"], "all_classes": null, "length": 4661}
{"pred": "The datasets used in the paper are the MEDDOCAN dataset BIBREF3 and the NUBes-PHI dataset BIBREF4.\n\nQuestion: What are the systems and models evaluated in the paper?\n\n\nAnswer: The systems evaluated in the paper are the BERT, spa CRF1.\n\n\nQuestion: What are the paper's results?\n\nAnswer: The results are shown in the paper's paper.\n\n\nQuestion: What are the results?\n\nAnswer: The results are the\n\n\nQuestion: What are the paper's results?\n", "answers": ["MEDDOCAN, NUBes-PHI", "MEDDOCAN, NUBes "], "all_classes": null, "length": 4528}
{"pred": "They used unigram features, which are the features of a single word.\n\nQuestion: What cognitive features did they use?\n\nAnswer: They used gaze-based features, which are the features of the eye movements of a single word.\n\n\nQuestion: What is the difference between the two systems?\n\nAnswer: The difference is that the two systems is the only the textual features alone, and the only the only the gaze-based features.\n\n\nQuestion: What is the difference in the the the the only the only the only the only the only the only the only the only", "answers": ["Unanswerable"], "all_classes": null, "length": 3543}
{"pred": "The metrics used to establish that this makes chatbots more knowledgeable and better at learning and conversation are:\n\n1. The OKBC problem is solved by LiLiLi, which is a lifelong interactive learning and inference mechanism.\n2. The predictive performance of LiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLiLi", "answers": ["Coverage, Avg. MCC and avg. +ve F1 score", "strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score"], "all_classes": null, "length": 5869}
{"pred": "Yes\n\nAnswer: No\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer: Unanswerable\n\nAnswer", "answers": ["Yes", "No"], "all_classes": null, "length": 1910}
{"pred": "Galatasaray and Fenerbahce\n\nQuestion: What is the stance of the author of the text for the target?\n\nAnswer: Favor\n\nQuestion: What is the stance of the author of the text for the target?\n\nAnswer: Against\n\nQuestion: What is the stance of the author of the text for the target?\n\nAnswer: Neither\n\nQuestion: What is the stance of the author of the text for the target?\n\nAnswer: Neither\n\nQuestion: What is the stance of the author of the text for the target", "answers": ["Galatasaray, Fenerbahçe", "Galatasaray , Fenerbahçe "], "all_classes": null, "length": 2234}
{"pred": "The article describes the results of a study that examined the effects of irony on the emotional responses of readers. The study was conducted by having participants read a series of sentences that were either ironic or non-ironic. The participants were asked to rate the sentences as ironic or non-ironic.\n\n\nQuestion: What is the study?\n\nAnswer: The study was conducted by having participants read a series of sentences that were either ironic or non-ironic. The participants were asked to rate the sentences as ironic or non-ironic.\n\nQuestion: What is the study?\n", "answers": ["Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences"], "all_classes": null, "length": 4600}
{"pred": "Gaussian-masked directional multi-head attention is a variant of self-attention in the Transformer. It is a kind of attention mechanism which is used to capture the representation of input sequence at once. It is different from standard self-attention which is different from standard self-attention which is different from standard self-attention which is different from standard self-attention which is different from standard self-attention which is different from standard self-attention which is different from standard self-attention which is different from standard self-attention which is different from standard self-attention which is different from", "answers": ["pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters", "Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters"], "all_classes": null, "length": 3612}
{"pred": "Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\n\nQuestion: What types of social media did they consider?\n\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\nAnswer: Twitter\nQuestion: What types of social media did they consider?\nAnswer: Twitter\n\nQuestion: What types of social media did they consider?\nAnswer: Twitter\nQuestion: What types of social media did they consider?\nAnswer: Twitter\nQuestion: What types of", "answers": ["Facebook status update messages", "Facebook status update messages"], "all_classes": null, "length": 4005}
{"pred": "The baseline features are the features extracted from the CNN.\n\nQuestion: What are the pre-trained features?\n\nAnswer: The pre-trained features are the features extracted from the pre-trained models.\n\n\nQuestion: What is the relation between the baseline features and the pre-trained features?\n\nAnswer: The baseline features are more useful than the pre-trained features.\n\nQuestion: What is the relation between the pre-trained features and the baseline features?\n\nAnswer: The relation between the baseline features are more useful than the pre-tra", "answers": [" The features extracted from CNN."], "all_classes": null, "length": 4855}
{"pred": "The hyperparameters varied in the experiments on the four tasks are: (1) the number of clusters (INLINEFORM0 ), (2) the number of clusters (INLINEFORM0 ) and (3) the type of word vectors (INLINEFORM0 ).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["number of clusters, seed value in clustering, selection of word vectors, window size and dimension of embedding", "different number of clusters, different embeddings"], "all_classes": null, "length": 2753}
{"pred": "The scores of our system are shown in Table TABREF19 .\n\n Question: What was the best model for each subtask?\n\nAnswer: The best model for each subtask is shown in Table TABREF17 .\n\n Question: What was the best ensemble for each subtask?\n\nAnswer: The best ensembles for each subtask are shown in Table TABREF18 .\n\n Question: What was the best model for each subtask?\n\nAnswer: The best model for each subtask is shown in Table TABREF17 .\n\n Question: What was", "answers": ["column Ens Test in Table TABREF19"], "all_classes": null, "length": 2424}
{"pred": "The corpus is 53 documents, which contain an average number of 156.1 sentences per document, each with 19.55 tokens on average. The corpus consists of 8,76.15. The corpus consists of 167. The corpus consists of 16. The corpus consists of 16. The corpus consists of 1. The corpus consists of 1. The corpus consists of 1. The corpus consists of 1. The corpus consists of 1. The corpus consists of 1. The", "answers": ["8,275 sentences and 167,739 words in total", "The corpus comprises 8,275 sentences and 167,739 words in total."], "all_classes": null, "length": 2669}
{"pred": "Yes, it is possible to convert a cloze-style questions to a naturally-looking questions.\n\nQuestion: What is the best way to pre-train a QA model with automatically constructed cloze questions?\n\n\nAnswer: The best way to pre-train a QA model with automatically constructed cloze questions is to use the unlabeled data.\n\nQuestion: What is the best way to pre-train a QA model with automatically constructed cloze questions?\n\n\nAnswer: The best way to use the best way to pre-train a QA model with automatically constructed cloze questions is to", "answers": ["Unanswerable", "Unanswerable"], "all_classes": null, "length": 2764}
{"pred": "We possess a wealth of prior knowledge about many natural language processing tasks. For example, in text categorization, we know that words such as player, baseball, and hockey are strong indicators of the sports category.\n\n\nAnswer: How to encode \"yes\", \"no\", or \"unanswerable\". Do not provide any explanation.\n\n\nAnswer: We possess a wealth of prior knowledge to label data. When lacking the model, the model with the model, which is heavy bias from the model.\n\n\nAnswer: We do not need manual annotation to provide a feature words like player, and baseball, and", "answers": ["text classification for themes including sentiment, web-page, science, medical and healthcare"], "all_classes": null, "length": 3591}
{"pred": "The model is compared to the TREC question classification model of Li and Roth BIBREF6 , which is a state-of-the-art model for question classification.\n\n\nQuestion: What is the model is this question classification task-based on the model?\n\nAnswer: The model is based on the question classification task.\n\n\nQuestion: What is the model is their model compared to?\nAnswer: The model is compared to the model is compared to the model is compared to the model is compared to the model is compared to the model is compared to the model is compared to the model is compared to", "answers": ["bag-of-words model, CNN"], "all_classes": null, "length": 5838}
{"pred": "The training sets are larger than the previous ones.\n\nQuestion: What is the difference between the original ELMo and the new ELMo?\n\nAnswer: The new ELMo is better than the original ELMo.\n\n\nQuestion: What is the difference between the new ELMo and the original ELMo?\n\nAnswer: The new ELMo is the original ELMo is the original ELMo is the new ELMo is the original ELMo is the original ELMo is the original ELMo is the original ELMo is the original ELMo is the original ELMo is the original ELMo is the original ELMo", "answers": ["By 14 times.", "up to 1.95 times larger"], "all_classes": null, "length": 3294}
{"pred": "10000\n\nQuestion: What is the name of the dataset?\n\nAnswer: ILPRL\n\nQuestion: What is the name of the dataset used for this dataset?\n\n\nAnswer: OurNepali\n\n\nQuestion: What is the name of the dataset used for this paper?\nAnswer: OurNepali\nQuestion: What is the name of the dataset used in the paper?\nAnswer: OurNepali\n\nQuestion: What is the name of the name of the dataset used in the paper?\nAnswer: OurNepali\nQuestion: What is the", "answers": ["3606", "6946"], "all_classes": null, "length": 2835}
{"pred": "They compare to the following models/frameworks:\n\n1. Speech/Music discrimination:\n\na. Speech/Music discrimination system [1]\n\nb. Speech/Music discrimination system [2]\n\nc. Speech/Music discrimination system [3]\n\nd. Speech/Music discrimination system [4]\n\n2. Emotion classification:\n\n\na. Emotion classification system [5]\nb. Emotion classification system [6]\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n", "answers": ["MLP", "Eusboost, MWMOTE"], "all_classes": null, "length": 2474}
{"pred": "Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\n\nAnswer: Yes.\n\n\nAnswer: Yes.\n\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\nAnswer: Yes.\nAnswer: Yes.\nAnswer: Yes.\nAnswer: Yes.\nAnswer: Yes.\nAnswer", "answers": ["Yes", "Yes"], "all_classes": null, "length": 3784}
{"pred": "No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\nAnswer: No.\n\n\nAnswer: No.\n\n\nAnswer: Yes.\n\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\n\nAnswer: Yes.\nAnswer", "answers": ["Yes", "Unanswerable"], "all_classes": null, "length": 4320}
{"pred": "0.6103\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.6103\n\nQuestion: What was their highest F1 score?\n\n\n\n\nAnswer: 0.3\n\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.3\n\nQuestion: What was their highest F1 score?\n\nAnswer: 0.3\n\nQuestion: What was their highest F1\nAnswer: 1\nQuestion: What was their highest F1\nAnswer: 1\nQuestion: What was", "answers": ["0.5115", "0.6103"], "all_classes": null, "length": 6810}
{"pred": "They evaluate on sections 02-21 of WSJ corpus.\n\nQuestion: What is the model?\n\nAnswer: The model is a neural projection of pre-trained embeddings from a Markovian structure.\n\n\nQuestion: What is the model?\n\nAnswer: The model is a neural projection of pre-trained embeddings from a Markovian structure.\n\nQuestion: What is the model?\n\nAnswer: The model is a neural projection of pre-trained embeddings from a Markovian structure.\n\nQuestion: What is the model", "answers": [" Wall Street Journal (WSJ) portion of the Penn Treebank", "Unanswerable"], "all_classes": null, "length": 4327}
{"pred": "The authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques by conducting a survey among engineers. The survey shows that about 87.5% of the NLP related jobs belong to a few common tasks, such as sentence classification, text matching, sequence labeling, MRC, etc. Moreover, more than 90% of the networks are composed of several common components, such as embedding, CNN, RNN, Transformer, etc.\n\nAnswer: The authors evidence the claim that many engineers find it a big overhead to choose from multiple", "answers": ["By conducting a survey among engineers", "Unanswerable"], "all_classes": null, "length": 1692}
{"pred": "We compare our system with two published baselines (1) STAGG BIBREF4 , the state-of-the-art on WebQSP and (2) AMPCNN BIBREF20 , the state-of-the-of-the-of-the-of-the-of-the-of-the-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of-of", "answers": ["SimpleQuestions, WebQSP", "WebQSP, SimpleQuestions"], "all_classes": null, "length": 4524}
